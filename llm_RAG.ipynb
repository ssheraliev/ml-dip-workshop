{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c587e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import shutil\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82230e9b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Langchain core\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableBranch\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20ef086",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Langchain community\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af21b218",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Streamlit\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80cbf3d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f9b180",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Environment setup ---\n",
    "def _set_env(var: str):\n",
    "    \"\"\"Sets an environment variable if not already set, prompting the user interactively.\"\"\"\n",
    "    if not os.environ.get(var):\n",
    "        logger.warning(f\"Environment variable '{var}' not found.\")\n",
    "        try:\n",
    "            os.environ[var] = getpass.getpass(f\"Please enter your {var}: \")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Could not get input for {var}: {e}. Functionality requiring this key may fail.\")\n",
    "            os.environ[var] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38e863d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "logger.info(f\"Set TOKENIZERS_PARALLELISM to {os.environ['TOKENIZERS_PARALLELISM']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b7dd5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Knowledge Base and Web Serahc configuration ---\n",
    "URLS = [\n",
    "    \"https://developers.google.com/machine-learning/guides/rules-of-ml/\",\n",
    "    \"https://peps.python.org/pep-0008/\",\n",
    "    \"https://google.github.io/styleguide/pyguide.html\"\n",
    "]\n",
    "PERSIST_DIRECTORY = \"./chroma_db_nomic_v1_notebook_final\"\n",
    "EMBEDDING_MODEL = \"nomic-embed-text-v1.5\"\n",
    "LLM_MODEL = \"gemma3\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "# --- Environment Variables ---\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "\n",
    "TAVILY_API_KEY = os.environ.get(\"TAVILY_API_KEY\")\n",
    "\n",
    "if TAVILY_API_KEY:\n",
    "    logger.info(\"TAVILY_API_KEY is set for this notebook session.\")\n",
    "else:\n",
    "    logger.warning(\"TAVILY_API_KEY could not be obtained or is empty. Web search will be disabled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2773f5fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# The build function\n",
    "def build_vector_store(force_rebuild=False):\n",
    "    \"\"\"Data load, split, embed, and persists to Vector DB (Chroma for now, but will be refined with other Open Source Vector Database).\"\"\"\n",
    "    logger.info(\"Starting the vector store build process...\")\n",
    "    was_built = False\n",
    "\n",
    "    if os.path.exists(PERSIST_DIRECTORY):\n",
    "        if force_rebuild:\n",
    "            logger.warning(f\"Force rebuild requested. Removing existing vector store at '{PERSIST_DIRECTORY}'.\")\n",
    "            try:\n",
    "                shutil.rmtree(PERSIST_DIRECTORY)\n",
    "                logger.info(f\"Removed existing directory: {PERSIST_DIRECTORY}\")\n",
    "            except OSError as e:\n",
    "                logger.error(f\"Error removing directory {PERSIST_DIRECTORY}: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            logger.info(f\"Vector store already exists at '{PERSIST_DIRECTORY}'. Set force_rebuild=True to overwrite.\")\n",
    "            return True\n",
    "\n",
    "    # load documents\n",
    "    logger.info(f\"Loading documents from {len(URLS)} URLs...\")\n",
    "    try:\n",
    "        loader = WebBaseLoader(URLS, continue_on_failure=True)\n",
    "        docs = loader.load()\n",
    "        if not docs:\n",
    "             logger.error(\"Documents not were successfully loaded. Check URLs and network connection.\")\n",
    "             return False\n",
    "        logger.info(f\"Successfully loaded {len(docs)} base documents.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed during document loading: {e}\")\n",
    "        return False\n",
    "\n",
    "    # split documents\n",
    "    logger.info(f\"Splitting documents into chunks (size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP})...\")\n",
    "    try:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=CHUNK_SIZE,\n",
    "            chunk_overlap=CHUNK_OVERLAP\n",
    "        )\n",
    "        doc_splits = text_splitter.split_documents(docs)\n",
    "        logger.info(f\"Documents split into {len(doc_splits)} chunks.\")\n",
    "        if not doc_splits:\n",
    "            logger.error(\"No document chunks were created after splitting.\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to split documents: {e}\")\n",
    "        return False\n",
    "\n",
    "    # init embeddings\n",
    "    logger.info(f\"Initializing Nomic embeddings ('{EMBEDDING_MODEL}' with local inference)...\")\n",
    "    logger.warning(\"This might download the embedding model if not already cached (~0.5 GB).\")\n",
    "    try:\n",
    "        embeddings = NomicEmbeddings(model=EMBEDDING_MODEL, inference_mode=\"local\")\n",
    "        logger.info(\"Nomic embeddings initialized.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize embeddings: {e}\")\n",
    "        return False\n",
    "\n",
    "    # create | persist to Vector Database\n",
    "    logger.info(f\"Creating and persisting ChromaDB vector store at '{PERSIST_DIRECTORY}'...\")\n",
    "    try:\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=doc_splits,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=PERSIST_DIRECTORY\n",
    "        )\n",
    "        logger.info(\"Vector store created and persisted successfully.\")\n",
    "        was_built = True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create/persist vector store: {e}\")\n",
    "        return False\n",
    "\n",
    "    logger.info(\"Vector store build process finished.\")\n",
    "    return was_built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda54439",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- build process ---\n",
    "build_successful = False\n",
    "try:\n",
    "    build_successful = build_vector_store(force_rebuild=False)\n",
    "    if build_successful:\n",
    "        print(f\"Vector store build/check completed successfully. Ready at {PERSIST_DIRECTORY}\")\n",
    "    else:\n",
    "        print(\"Vector store build failed.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during the build execution block: {e}\")\n",
    "    logger.exception(\"Error in build execution block\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e727bdaf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# components for runtime queries\n",
    "components_loaded = False\n",
    "llm = None\n",
    "retriever = None\n",
    "web_search_tool = None\n",
    "router_llm = None\n",
    "embeddings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf21ddaf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- build validation phase ---\n",
    "if 'build_successful' not in locals() and 'build_successful' not in globals():\n",
    "    logger.error(\"The variable 'build_successful' is not defined.\")\n",
    "    logger.error(\"run the cell in 'indexing phase' before running this cell.\")\n",
    "elif build_successful:\n",
    "    logger.info(\"Build successful, proceeding to initialize runtime components...\")\n",
    "    try:\n",
    "        logger.info(f\"Initializing Nomic embeddings ({EMBEDDING_MODEL}, local)...\")\n",
    "        embeddings = NomicEmbeddings(model=EMBEDDING_MODEL, inference_mode=\"local\")\n",
    "\n",
    "        logger.info(f\"Loading Chroma vector store from {PERSIST_DIRECTORY}...\")\n",
    "        if not os.path.exists(PERSIST_DIRECTORY):\n",
    "             logger.error(f\"Vector store directory not found at {PERSIST_DIRECTORY}, though build was marked successful?\")\n",
    "             raise FileNotFoundError(f\"Vector store not found at {PERSIST_DIRECTORY}\")\n",
    "        vectorstore = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embeddings)\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3}) # top 3 chunks\n",
    "        logger.info(\"Vector store and retriever loaded.\")\n",
    "\n",
    "        logger.info(f\"Initializing Ollama LLM ({LLM_MODEL})...\")\n",
    "        llm = Ollama(model=LLM_MODEL, temperature=0)\n",
    "        llm.invoke(\"Respond briefly: OK\") # connection validation\n",
    "        logger.info(\"Ollama LLM connection verified.\")\n",
    "        router_llm = llm\n",
    "\n",
    "        # init Tavily - web search\n",
    "        if TAVILY_API_KEY:\n",
    "            web_search_tool = TavilySearchResults(k=3)\n",
    "            logger.info(\"Tavily Search tool initialized.\")\n",
    "        else:\n",
    "            web_search_tool = None\n",
    "            logger.warning(\"web search NOT initialized (API key not available).\")\n",
    "\n",
    "        components_loaded = True\n",
    "        logger.info(\"All available runtime components initialized successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error initializing runtime components: {e}\")\n",
    "        components_loaded = False\n",
    "else:\n",
    "    components_loaded = False\n",
    "    logger.error(\"Cannot initialize runtime components because the vector store build failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfbc8bc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if components_loaded:\n",
    "    print(\"Runtime components are loaded and ready.\")\n",
    "else:\n",
    "    print(\"Runtime components failed to load.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463edd9b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Router, RAG chain, web search, and branching logic\n",
    "def format_tavily_results(results):\n",
    "    \"\"\"Tavily search results formatted into a readable string.\"\"\"\n",
    "    if not results:\n",
    "        return \"No relevant information found via web search.\"\n",
    "    try:\n",
    "        # results is iterable and items are dictionaries\n",
    "        if not isinstance(results, list): return \"Received non-list results from Tavily.\"\n",
    "        summary = \"\\n\\n\".join(\n",
    "            f\"URL: {res.get('url', 'N/A')}\\nContent: {res.get('content', 'N/A')}\"\n",
    "            for res in results if isinstance(res, dict)\n",
    "        )\n",
    "        return summary if summary else \"Found results, but could not extract content.\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error formatting Tavily results: {e}\")\n",
    "        return \"Error processing search results.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0a01e7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Chains ---\n",
    "full_chain = None\n",
    "\n",
    "if components_loaded:\n",
    "    logger.info(\"Defining Langchain chains...\")\n",
    "    try:\n",
    "        # Router Chain\n",
    "        router_prompt_template = \"\"\"Given the user query, determine if it is best answered using internal knowledge about Python Style Guides (PEP8, Google Style Guide) and ML best practices (Google's Rules of ML), or if it requires a real-time web search for general programming topics, specific code examples not related to style/ML rules, current events, or very recent information.\n",
    "Respond only with the word 'vectorstore' or 'web_search'.\n",
    "\n",
    "User Query: {question}\n",
    "Decision:\"\"\"\n",
    "        router_prompt = PromptTemplate.from_template(router_prompt_template)\n",
    "        router = (\n",
    "            {\"question\": RunnablePassthrough()}\n",
    "            | router_prompt\n",
    "            | router_llm\n",
    "            | StrOutputParser()\n",
    "            | RunnableLambda(lambda x: x.strip().lower().replace(\"'\", \"\").replace('\"', '')) # Clean output\n",
    "        )\n",
    "\n",
    "        # RAG Chain\n",
    "        rag_prompt_template = \"\"\"You are coding assistant specializing in Python style (PEP8, Google Style Guide) and ML best practices (Google's Rules of ML). Answer the user's question based *only* on the following provided context. If the context doesn't contain the answer, state that the specific information isn't available in the provided knowledge base. Do not use external knowledge.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        rag_prompt = ChatPromptTemplate.from_template(rag_prompt_template)\n",
    "        rag_chain = (\n",
    "            RunnablePassthrough.assign(\n",
    "                context= RunnablePassthrough.assign(question=lambda x: x[\"question\"])\n",
    "                         | (lambda x: x[\"question\"])\n",
    "                         | retriever\n",
    "                         | (lambda docs: \"\\n\\n\".join(doc.page_content for doc in docs))\n",
    "            )\n",
    "            | rag_prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "\n",
    "        # Web Search Chain\n",
    "        if web_search_tool:\n",
    "            web_search_prompt_template = \"\"\"You are a helpful Python coding assistant. Analyze the user's question and the provided web search results.\n",
    "\n",
    "1.  **Determine Intent:** Is the user asking for an explanation, or for code implementation (e.g., \"How to write...\", \"Implement...\", \"Code for...\")?\n",
    "2.  **Synthesize Answer:**\n",
    "    *   If the user wants an explanation, provide a clear summary based on the context.\n",
    "    *   If the user wants code: Generate clean, functional Python code based on the information in the web search results and the user's request.\n",
    "3.  **Apply Style:** When generating Python code, make a best effort to adhere to the Google Python Style Guide (e.g., clear variable names, docstrings for functions/classes, reasonable line lengths, comments where necessary).\n",
    "4.  **Handle Insufficient Info:** If the search results are irrelevant or insufficient to fulfill the request (either explanation or code), clearly state that.\n",
    "\n",
    "Web Search Results:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "            web_search_prompt = ChatPromptTemplate.from_template(web_search_prompt_template)\n",
    "            web_chain = (\n",
    "                RunnablePassthrough.assign(\n",
    "                     context= RunnablePassthrough.assign(question=lambda x: x[\"question\"])\n",
    "                              | (lambda x: x[\"question\"])\n",
    "                              | web_search_tool\n",
    "                              | RunnableLambda(format_tavily_results)\n",
    "                )\n",
    "                | web_search_prompt\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    "            )\n",
    "            logger.info(\"Web search chain defined.\")\n",
    "        else:\n",
    "            web_chain = RunnableLambda(lambda x: \"Web search is not available - missing Tavily API key.\")\n",
    "            logger.warning(\"Web search chain is disabled (no API key).\")\n",
    "\n",
    "        # Branching\n",
    "        def decide_chain(route_info):\n",
    "            \"\"\"Decide which chain to run based on router output.\"\"\"\n",
    "            question = route_info.get(\"question\", \"N/A\")\n",
    "            route_decision = route_info.get(\"route_decision\", \"\").lower()\n",
    "            logger.debug(f\"Deciding chain for question '{question[:50]}...'. Router decision: '{route_decision}'\")\n",
    "\n",
    "            if \"vectorstore\" in route_decision:\n",
    "                logger.info(f\"Routing to RAG chain for question: {question[:50]}...\")\n",
    "                return rag_chain\n",
    "            elif web_search_tool:\n",
    "                logger.info(f\"Routing to Web Search chain for question: {question[:50]}...\")\n",
    "                return web_chain\n",
    "            else:\n",
    "                logger.warning(f\"Routing defaulted away from web search (disabled) for question: {question[:50]}...\")\n",
    "                return RunnableLambda(lambda x: \"Web search was intended but is disabled.\")\n",
    "\n",
    "\n",
    "        # original question and the routing decision\n",
    "        routing_chain = RunnablePassthrough.assign(\n",
    "            route_decision = {\"question\": RunnablePassthrough()} | router\n",
    "        )\n",
    "        \n",
    "        full_chain = routing_chain | RunnableLambda(decide_chain)\n",
    "\n",
    "        logger.info(\"Langchain chains defined successfully.\")\n",
    "        print(\"Langchain chains are defined and ready for testing.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Error defining Langchain chains.\")\n",
    "        print(f\"Error defining Langchain chains: {e}\")\n",
    "        full_chain = None\n",
    "else:\n",
    "    logger.error(\"Skipping chain definition as components failed to load.\")\n",
    "    print(\"Skipping chain definition as components failed to load.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba36ef14",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# validation of chains\n",
    "if full_chain:\n",
    "    print(\"\\n--- Running tests ---\")\n",
    "    test_queries = {\n",
    "        \"Merge Sort\": \"How to write Merge Sort Algorithm?\",\n",
    "        \"ML Rule\": \"How to implement Regression Machine Learning Model?\"\n",
    "    }\n",
    "\n",
    "    for name, query in test_queries.items():\n",
    "        print(f\"\\n--- Testing Query ({name}): '{query}' ---\")\n",
    "        try:\n",
    "            \n",
    "            response = full_chain.invoke({\"question\": query})\n",
    "            print(f\"Full Chain Response:\\n{response}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"!!! Error testing query '{query}': {e}\")\n",
    "            logger.exception(f\"Error during interactive test for query: {query}\")\n",
    "\n",
    "    print(\"--- Chain tests completed ---\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping testing as chains could not be defined.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
